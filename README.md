# Lingu67 - AI Memory Assistant

A local AI assistant with persistent memory, GPU-accelerated LLM inference, and vector-based retrieval.

## ğŸ—ï¸ Project Structure

```
lingu67/
â”œâ”€â”€ chatbot/                # Memory Assistant Chatbot
â”‚   â”œâ”€â”€ adapters/           # LLM & Embedding wrappers
â”‚   â”œâ”€â”€ agents/             # Specialized workers (memory, file, profile, rag)
â”‚   â”œâ”€â”€ orchestrator/       # Central coordinator & routing
â”‚   â”œâ”€â”€ interfaces/         # CLI & Web entry points
â”‚   â”œâ”€â”€ chatbot_ui.py       # [Legacy] Original Flask web UI
â”‚   â””â”€â”€ memory_assistant.py # [Legacy] Original CLI
â”‚
â”œâ”€â”€ server/                 # Shared LLM Infrastructure
â”‚   â”œâ”€â”€ model_server.py     # FastAPI server (GPU LLM + embeddings)
â”‚   â”œâ”€â”€ local_client.py     # HTTP client library
â”‚   â””â”€â”€ rag_handler.py      # RAG utilities
â”‚
â”œâ”€â”€ memory/                 # ğŸ†• Memory Framework
â”‚   â”œâ”€â”€ core.py             # MemoryManager orchestrator
â”‚   â”œâ”€â”€ short_term.py       # Volatile context (policies apply)
â”‚   â”œâ”€â”€ long_term.py        # Qdrant-backed persistent storage
â”‚   â”œâ”€â”€ feature_memory.py   # JSON facts with history
â”‚   â”œâ”€â”€ llm_manager.py      # LLM-driven memory decisions
â”‚   â”œâ”€â”€ policies.py         # Retention, decay, compression
â”‚   â”œâ”€â”€ scopes.py           # User/global/session isolation
â”‚   â””â”€â”€ README.md           # Full documentation
â”‚
â”œâ”€â”€ models/                 # LLM Model Files
â”‚   â””â”€â”€ qwen2.5-3b-instruct-q4_k_m.gguf
â”‚
â”œâ”€â”€ data/                   # Persistent Data
â”‚   â”œâ”€â”€ qdrant_local/       # Vector database
â”‚   â”œâ”€â”€ user_profile.json   # User facts
â”‚   â””â”€â”€ conversation_log.txt
â”‚
â”œâ”€â”€ llama.cpp/              # llama-server binaries (GPU)
â”œâ”€â”€ config.py               # Global configuration
â””â”€â”€ requirements.txt
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Start Model Server
```bash
python server/model_server.py
```
This starts:
- llama-server on port 8080 (GPU LLM inference)
- FastAPI proxy on port 8000 (embeddings + API)

### 3. Start Chatbot

**New Modular Version (Recommended):**
```bash
# CLI
python -m chatbot.interfaces.cli

# Web UI
python -m chatbot.interfaces.web.app
```

**Legacy Version:**
```bash
python chatbot/chatbot_ui.py  # Port 7860
```

Open http://localhost:5000 (new) or http://localhost:7860 (legacy) in your browser.

## ğŸ§  Memory Framework

The new modular memory system provides three distinct memory types:

| Memory Type | Storage | Policies | Purpose |
|-------------|---------|----------|---------|
| **Short-term** | In-memory | Yes (token limit, decay) | Recent conversation context |
| **Long-term** | Qdrant DB | No (permanent) | All past conversations (semantic search) |
| **Feature** | JSON files | No | User facts with history tracking |

### Quick Usage

```python
from memory import MemoryManager

mm = MemoryManager(scope="user:123", llm_client=llm)

# Add conversation
mm.add_turn("What's my name?", "Your name is John")

# Get context for LLM
context = mm.get_context("Tell me about myself")

# Set/get facts
mm.set_fact("language", "Python")
print(mm.get_fact("language"))  # "Python"
```

### Benchmarks

| Operation | Throughput | p50 Latency |
|-----------|------------|-------------|
| Short-term write | 95,724 ops/sec | 0.006ms |
| Short-term read | 15,256 ops/sec | 0.065ms |
| Semantic search | ~57 ops/sec | 17.6ms |
| End-to-end turn | ~2.5 ops/sec | 388ms |

See `memory/README.md` for full documentation.

## ğŸ“ File Upload

- Upload files via the web UI drawer
- Files are chunked and embedded for semantic search
- Ask questions like "summarize chapter 3"

## âš™ï¸ Configuration

All paths and settings are in `config.py`:

```python
from config import (
    MODEL_SERVER_URL,    # http://localhost:8000
    QDRANT_PATH,         # data/qdrant_local
    LLM_MODEL_PATH,      # models/qwen2.5-3b-...
)
```

## ğŸ§ª Testing

```bash
# Memory framework tests (80 total)
python memory/tests.py           # 55 unit tests
python memory/extreme_tests.py   # 10 stress tests
python memory/llm_integration_test.py  # 5 LLM tests
python memory/llm_stress_test.py       # 10 edge cases

# Benchmarks
python memory/benchmark.py
```

## ğŸ”§ Adding New Subprojects

```python
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from server.local_client import LocalLLMClient
from memory import MemoryManager
from config import MODEL_SERVER_URL

client = LocalLLMClient(MODEL_SERVER_URL)
mm = MemoryManager(scope="user:myapp", llm_client=client)
```

## ğŸ“¦ Tech Stack

- **LLM**: Qwen 2.5 3B (GGUF) via llama.cpp
- **Embeddings**: SentenceTransformer (all-MiniLM-L6-v2)
- **Vector DB**: Qdrant (local file mode)
- **Memory**: Custom framework (short-term, long-term, feature)
- **Web UI**: Flask + vanilla JS
- **API**: FastAPI
- **GPU**: CUDA 12.4 (RTX 3050)

## ğŸ“„ License

MIT
