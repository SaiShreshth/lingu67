# Lingu67 - AI Memory Assistant

A local AI assistant with persistent memory, GPU-accelerated LLM inference, and vector-based retrieval. Features a modular architecture with specialized agents and a comprehensive LLM memory framework.

## ğŸ—ï¸ Project Structure

```
lingu67/
â”œâ”€â”€ chatbot/                    # Memory Assistant Chatbot
â”‚   â”œâ”€â”€ adapters/               # LLM & Embedding wrappers
â”‚   â”‚   â”œâ”€â”€ llm_adapter.py      # LLM client wrapper
â”‚   â”‚   â””â”€â”€ embedding_adapter.py # Embedding client wrapper
â”‚   â”œâ”€â”€ agents/                 # Specialized workers
â”‚   â”‚   â”œâ”€â”€ base.py             # Base agent class
â”‚   â”‚   â”œâ”€â”€ memory_agent.py     # Memory operations
â”‚   â”‚   â”œâ”€â”€ file_agent.py       # File handling
â”‚   â”‚   â”œâ”€â”€ profile_agent.py    # User profile management
â”‚   â”‚   â””â”€â”€ rag_agent.py        # RAG retrieval
â”‚   â”œâ”€â”€ orchestrator/           # Central coordinator
â”‚   â”‚   â”œâ”€â”€ core.py             # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ router.py           # Intent routing
â”‚   â”‚   â””â”€â”€ context.py          # Session context
â”‚   â”œâ”€â”€ interfaces/             # Entry points
â”‚   â”‚   â”œâ”€â”€ cli.py              # Command-line interface
â”‚   â”‚   â””â”€â”€ web/                # Flask web UI
â”‚   â”‚       â”œâ”€â”€ app.py          # Flask application
â”‚   â”‚       â”œâ”€â”€ routes.py       # API routes
â”‚   â”‚       â””â”€â”€ templates.py    # HTML templates
â”‚   â”œâ”€â”€ old/                    # Legacy versions
â”‚   â”‚   â”œâ”€â”€ chatbot_ui.py       # Original Flask UI
â”‚   â”‚   â””â”€â”€ memory_assistant.py # Original CLI
â”‚   â””â”€â”€ README.md               # Chatbot documentation
â”‚
â”œâ”€â”€ memory/                     # LLM Memory Framework
â”‚   â”œâ”€â”€ core.py                 # MemoryManager orchestrator
â”‚   â”œâ”€â”€ stores/                 # Memory storage
â”‚   â”‚   â”œâ”€â”€ short_term.py       # Volatile context (policies apply)
â”‚   â”‚   â”œâ”€â”€ long_term.py        # Qdrant-backed persistent storage
â”‚   â”‚   â””â”€â”€ feature_memory.py   # JSON facts with history
â”‚   â”œâ”€â”€ managers/               # Memory management
â”‚   â”‚   â”œâ”€â”€ llm_manager.py      # LLM-driven decisions
â”‚   â”‚   â””â”€â”€ policies.py         # Retention, decay, compression
â”‚   â”œâ”€â”€ utils/                  # Utilities
â”‚   â”‚   â”œâ”€â”€ scopes.py           # User/global/session isolation
â”‚   â”‚   â””â”€â”€ helpers.py          # Token counting, JSON utilities
â”‚   â”œâ”€â”€ backends/               # Storage backends
â”‚   â”‚   â””â”€â”€ qdrant_backend.py   # Qdrant vector DB wrapper
â”‚   â”œâ”€â”€ tests/                  # Test suite (80 tests)
â”‚   â”‚   â”œâ”€â”€ unit_tests.py       # 55 unit tests
â”‚   â”‚   â”œâ”€â”€ extreme_tests.py    # 10 stress tests
â”‚   â”‚   â”œâ”€â”€ llm_integration_test.py # 5 LLM tests
â”‚   â”‚   â”œâ”€â”€ llm_stress_test.py  # 10 edge case tests
â”‚   â”‚   â””â”€â”€ benchmark.py        # Performance benchmarks
â”‚   â””â”€â”€ README.md               # Full memory documentation
â”‚
â”œâ”€â”€ server/                     # Shared LLM Infrastructure
â”‚   â”œâ”€â”€ model_server.py         # FastAPI server (GPU LLM + embeddings)
â”‚   â”œâ”€â”€ local_client.py         # HTTP client library
â”‚   â””â”€â”€ rag_handler.py          # RAG utilities & file ingestion
â”‚
â”œâ”€â”€ chess/                      # Chess Game with AI
â”‚   â”œâ”€â”€ app.py                  # Flask chess web UI
â”‚   â”œâ”€â”€ game.py                 # Game logic
â”‚   â”œâ”€â”€ stockfish_client.py     # Stockfish AI integration
â”‚   â””â”€â”€ chess_client.py         # LLM analysis
â”‚
â”œâ”€â”€ config.py                   # Global configuration
â””â”€â”€ requirements.txt            # Python dependencies
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Start Model Server
```bash
python server/model_server.py
```
This starts:
- llama-server on port 8080 (GPU LLM inference)
- FastAPI proxy on port 8000 (embeddings + API)

### 3. Start Chatbot

**New Modular Version (Recommended):**
```bash
# Command Line
python -m chatbot.interfaces.cli

# Web UI
python -m chatbot.interfaces.web.app
```

**Legacy Version:**
```bash
python chatbot/old/chatbot_ui.py  # Port 7860
```

Open http://localhost:5000 (new) or http://localhost:7860 (legacy)

---

## ğŸ§  Memory Framework

A comprehensive LLM memory system with three distinct memory types:

| Memory Type | Storage | Policies | Purpose |
|-------------|---------|----------|---------|
| **Short-term** | In-memory | âœ… Token limit, decay | Recent conversation context |
| **Long-term** | Qdrant DB | âŒ Permanent | All conversations (semantic search) |
| **Feature** | JSON files | âŒ No | User facts with history tracking |

### Usage

```python
from memory import MemoryManager

mm = MemoryManager(scope="user:123", llm_client=llm)

# Add conversation
mm.add_turn("What's my name?", "Your name is John")

# Get context for LLM
context = mm.get_context("Tell me about myself")

# Set/get facts
mm.set_fact("language", "Python")
print(mm.get_fact("language"))  # "Python"

# Search long-term memory
results = mm.search_long_term("Python programming")
```

### Performance Benchmarks

| Operation | Throughput | p50 Latency |
|-----------|------------|-------------|
| Short-term write | 95,724 ops/sec | 0.006ms |
| Short-term read | 15,256 ops/sec | 0.065ms |
| Feature read | 2.6M ops/sec | <0.001ms |
| Semantic search | ~57 ops/sec | 17.6ms |
| End-to-end turn | ~2.5 ops/sec | 388ms |

See [memory/README.md](memory/README.md) for full documentation.

---

## ğŸ¤– Chatbot Architecture

The modular chatbot uses specialized agents coordinated by an orchestrator:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ORCHESTRATOR                    â”‚
â”‚  â€¢ Intent detection & routing                    â”‚
â”‚  â€¢ Session context management                    â”‚
â”‚  â€¢ Response coordination                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ADAPTERS      â”‚    â”‚    AGENTS      â”‚
    â”‚  â€¢ LLM Client   â”‚    â”‚  â€¢ Memory      â”‚
    â”‚  â€¢ Embeddings   â”‚    â”‚  â€¢ File        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â€¢ Profile     â”‚
                           â”‚  â€¢ RAG         â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â™Ÿï¸ Chess Module

Play chess against Stockfish with LLM-powered move analysis:

```bash
python chess/app.py
```
- Web UI at http://localhost:5001
- Stockfish AI opponent
- LLM move explanations

---

## ğŸ“ File Upload

- Upload files via the web UI drawer
- Files are chunked and embedded for semantic search
- Ask questions like "summarize chapter 3"

---

## âš™ï¸ Configuration

All paths and settings are in `config.py`:

```python
from config import (
    MODEL_SERVER_URL,    # http://localhost:8000
    QDRANT_PATH,         # data/qdrant_local
    LLM_MODEL_PATH,      # models/qwen2.5-3b-...
    LLAMA_SERVER_PATH,   # llama.cpp binaries
)
```

---

## ğŸ§ª Testing

```bash
# Memory framework tests (80 total)
python memory/tests/unit_tests.py          # 55 unit tests
python memory/tests/extreme_tests.py       # 10 stress tests
python memory/tests/llm_integration_test.py # 5 LLM tests
python memory/tests/llm_stress_test.py     # 10 edge cases

# Benchmarks
python memory/tests/benchmark.py
```

---

## ğŸ”§ Creating New Subprojects

```python
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from server.local_client import LocalLLMClient
from memory import MemoryManager
from config import MODEL_SERVER_URL

# Initialize
client = LocalLLMClient(MODEL_SERVER_URL)
mm = MemoryManager(scope="user:myapp", llm_client=client)

# Use memory
mm.add_turn(user_message, assistant_response)
context = mm.get_context(user_message)
```

---

## ğŸ“¦ Tech Stack

| Component | Technology |
|-----------|-----------|
| **LLM** | Qwen 2.5 3B (GGUF) via llama.cpp |
| **Embeddings** | SentenceTransformer (all-MiniLM-L6-v2) |
| **Vector DB** | Qdrant (local file mode) |
| **Memory** | Custom framework (short-term, long-term, feature) |
| **Web UI** | Flask + vanilla JS |
| **API** | FastAPI |
| **GPU** | CUDA 12.4 (RTX 3050) |

---

## ğŸ“„ License

MIT
